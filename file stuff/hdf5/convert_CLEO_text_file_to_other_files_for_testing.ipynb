{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert CLEO file to different formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cleo_tools as cleo\n",
    "import numpy as np\n",
    "import h5py\n",
    "from math import sqrt\n",
    "\n",
    "import zipfile\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sn\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "infilename = 'data31_100k_LARGE.dat'\n",
    "#infilename = 'data31_500k.dat'\n",
    "#infilename = 'data31.dat'\n",
    "\n",
    "\n",
    "infile = open(infilename)\n",
    "collisions = cleo.get_collisions(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parse the data so that we can have\n",
    "# \n",
    "# Number of each type of particle.\n",
    "# Store the information about the particles in 2D blocks. \n",
    "#     This works if all the data are the same file type (e.g. floats)\n",
    "\n",
    "lp = []\n",
    "lk = []\n",
    "lm = []\n",
    "le = []\n",
    "lph = []\n",
    "\n",
    "pis = []\n",
    "kas = []\n",
    "mus = []\n",
    "els = []\n",
    "phs = []\n",
    "\n",
    "for collision in collisions:\n",
    "    \n",
    "    pions,kaons,muons,electrons,photons = collision\n",
    "    \n",
    "    npi = len(pions)\n",
    "    nka = len(kaons)\n",
    "    nmu = len(muons)\n",
    "    nel = len(electrons)\n",
    "    nph = len(photons)\n",
    "    \n",
    "    lp.append(npi)\n",
    "    lk.append(nka)\n",
    "    lm.append(nmu)\n",
    "    le.append(nel)\n",
    "    lph.append(nph)\n",
    "    \n",
    "    for pion in pions:\n",
    "        \n",
    "        pis.append(pion)\n",
    "    \n",
    "    for kaon in kaons:\n",
    "        \n",
    "        kas.append(kaon)\n",
    "        \n",
    "    for muon in muons:\n",
    "        \n",
    "        mus.append(muon)\n",
    "    \n",
    "    for electron in electrons:\n",
    "        \n",
    "        els.append(electron)\n",
    "    \n",
    "    for photon in photons:\n",
    "        \n",
    "        phs.append(photon)\n",
    "    \n",
    "    #print \"event %i: |\" % (i), \"pions: %i | kaons: %i | muons: %i | electrons: %i | photons: %i | \" % (npi,nka,nmu,nel,nph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data31_100k_LARGE_NO_COMPRESSION.hdf5\n",
      "data31_100k_LARGE_GZIP.hdf5\n",
      "data31_100k_LARGE_LZF.hdf5\n"
     ]
    }
   ],
   "source": [
    "# Make the HDF file with different compressions\n",
    "\n",
    "comp_tags = ['NO_COMPRESSION','GZIP','LZF']\n",
    "algs = [None,'gzip','lzf']\n",
    "compression_levels = [None,9,None]\n",
    "\n",
    "\n",
    "for tag,alg,opt in zip(comp_tags,algs,compression_levels):\n",
    "    \n",
    "    end_tag = \"_%s.hdf5\" % (tag)\n",
    "    outfilename = infilename.replace('.dat',end_tag)\n",
    "    \n",
    "    print outfilename\n",
    "\n",
    "    with h5py.File(outfilename,'a') as hf:\n",
    "        hf.create_dataset('npions',data=lp,compression=alg,compression_opts=opt)\n",
    "        hf.create_dataset('nkaons',data=lk,compression=alg,compression_opts=opt)\n",
    "        hf.create_dataset('nmuons',data=lm,compression=alg,compression_opts=opt)\n",
    "        hf.create_dataset('nelectrons',data=le,compression=alg,compression_opts=opt)\n",
    "        hf.create_dataset('nphotons',data=lph,compression=alg,compression_opts=opt)\n",
    "\n",
    "        g1 = hf.create_group('particles')\n",
    "\n",
    "        g1.create_dataset('pions',data=pis,compression=alg,compression_opts=opt)\n",
    "        g1.create_dataset('kaons',data=kas,compression=alg,compression_opts=opt)\n",
    "        g1.create_dataset('muons',data=mus,compression=alg,compression_opts=opt)\n",
    "        g1.create_dataset('electrons',data=els,compression=alg,compression_opts=opt)\n",
    "        g1.create_dataset('photons',data=phs,compression=alg,compression_opts=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a zipfile \n",
    "\n",
    "zipfilename = \"%s.zip\" % (infilename.split('.dat')[0])\n",
    "zf = zipfile.ZipFile(zipfilename,'w')\n",
    "zf.write(infilename.split(',dat')[0],compress_type=zipfile.ZIP_DEFLATED)\n",
    "zf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
